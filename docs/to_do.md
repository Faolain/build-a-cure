# Summary

- now youve built some powerful functions, some of which need completion, debugging & generalization
    - the important next step is hooking up clause/relationship identification to those functions
    - since those are the foundation for the other more important semantic functions in get_medical_objects and get_conceptual_objects
    - key requirements to complete next:
      - find_type
      - find_definition - can pull from dataset for a type to build definition
      - consolidate pattern identification functions
      - test & update find_clause/relation logic
      - test & update derive_patterns logic to build a full pattern index to implement with clause/relation logic

- the relationships between object type layers (structural, conceptual, medical) means that you can re-use logic across layers:
    - example:
      - stressor identification logic can overlap with change/variance/variable identification logic
    - other type mappings to generalize:
      condition = state, symptom = function = side_effects, function = relationship, synthesis = build process, structure = pattern
    - at some point you need to identify this system of relationships between object types & layers 
      so that generation of additional layer interfaces is possible (physics system, math system, chemical system, in addition to medical/bio system)

- you can work on other tasks for a while before completing relationship/clause/pattern fitting:
  - insight identifier
  - dose prediction for a patient
  - fetch contraindications for a drug (find nth-degree side effects, outputs, high dose impact if not metabolized, conditions, interacting/synergistic/neutralizing drugs)
  - find most common adjacent compound (to make it likeliest that the person can find the common compound and synthesize the version they need)
  - fetch synthesis instructions for a drug from most common adjacent compound
  - treatment component identification function
  - drug reaction predictor
  - compound search from smile formula


# Sources

  - check chembl search if you can search for a condition & return molecules known to treat it
  - chembl similarity function can tell you how likely it is that the generated compound mimics functionality of another compound
  - resolve local_database, get_data_store, derive_pattern, get_data_from_source logic & calls
  - pull these properties for compounds on wiki
  - find source of bio keywords & synonyms


# Structural Objects

  - add complications object when querying treatments
  - resolve \n separator when not used as a new line separator
  - store definitions
  - fix conjugation
  - fix return from type_patterns

  - when evaluating interactions, check for other compounds that interfere with metabolism & de/activation (cytochromes it targets, liver enzymes it assists), 
    which can increase or decrease blood ratio of a drug
  - look for processes/intake of nutrients that could combine to form other compounds (berberine) given the output health factors (stable blood sugar)
  - fix order of assembled combinations:
      get_alts: all_alts [['suppose', 'assumed', 'thought'], ['DT', 'PDT', 'WDT', 'TO', 'PP', 'CC', 'IN'], ' that ', ['suppose', 'assumed', 'thought'], ' that']
      get_all_versions |suppose thought assumed| that DPC |suppose thought assumed| that
  - remove plural tags once you finish singularize function
  - make sure apply_pattern_map explores all versions of line, but returns one new line
  - add common patterns that have more than one index type to all index type lists - 'x of y', 'phrase of phrase', etc
  - identify lists in sentence and surround with parenthesis if embedded or insert as examples of an object ('such as', 'like', 'as in'), 'found in', 'including', 'having'
  - find functions should have definition logic & logic to rule out other types & type-specific logic since they're used as a backup to pattern-matching
    the order of find_* function application can take the place of this, if patterns are comprehensive enough
  - add pattern to standardize verb-subject to subject-verb: 'V DET noun_phrase ... ?' => 'DET noun_phrase V ...'
  - finish function to combine functions by intent get_net_impact(functions) & combined operators
  - verify that if not response false check is same as youve been using

  pattern alts:
  - repeated options shouldnt happen within an alt set: |NNS NNS VBZ2 VBZ3 NNS| 
  - implement pattern-mix matching to mix & match patterns of various types to find key patterns with mixed types not generated by current logic

  processing order:
  - examine iterations (lists/keys()/items()/config/if conditions) that determine processing order: (supported_pattern_variables, pos_tags, all_pattern_version_types, reversed keys, etc)
    - when identifying all objects, order can be from low to high
    - when classifying specific objects, order should be from high to low - return first match, or adjust line being analyzed with replacement for each match, starting with longest matches first
    - add ordered pos-tagging pattern_map to apply preference order to correct incorrectly identified word pos - isolate which tags would be identified as other objects first

  clause identification: 
  - add ordering logic in find_clause for special clause keywords:
    - 'as' can mean 'like', 'while', or 'because'
    - 'by' can indicate a process/mechanism "it works by doing x", "as"

  - support conversion between pos types like 'verb-to-noun':
    - 'subject1 verb clause because subject2 verb clause' => 'subject2 verb-to-noun causes subject1 verb-to-noun'
    - 'the process activated x because y inhibits b' => 'y b-inhibition causes the process to activate x' => 'y b-inhibition enables process to activate x'

  - fix rows csv format & read/save delimiter handling for get_objects - we are storing patterns with 'pattern_match1::match2::match3' syntax for example
  - write function to get semantic props of compounds (bio-availability, activation in the host species, etc) & get_common_property between objects
  - integrate conditions/symptoms and treatments/compounds schemas (this would be a nice way to test get_attribute function to find differentiating props)
  - remove len(0) checks for lists when possible & consolidate excessive chained response checks
  - make sure youre not assigning scores or other calculated numbers as dict keys or other identifiers anywhere 
  - add keyword processing to apply_find_function 

# Functions

  - generating find/build functions:
    - prioritize defining & assembling type definitions, both with configuration & programmatically using various sources
      - find source types from which others can be derived
      - apply get_definition to get the definition of those source types
      - then apply transform function for each pair to generate a find function for a type given the find function of an adjacent type
      - find functions use primarily patterns & definitions
      - create function that uses definition to generate patterns given variable values/types/metadata
      - build functions can also be generated using the type definition
      - after generating, functions should be checked for non-identifying factors that dont differentiate them across types

  - generating apply functions:
    - match/align/fit a structure to another structure

  - standardize terms: shape/structure, model/perspective/filter/standard/interface, intent/method/function/rule, path/route, metadata/attribute/variable/property, object/entity, type/class/category, variance/randomness/chaos/entropy
  - add variable accretion patterns (how an object becomes influenced by a new variable, complex system interaction patterns, etc)
  - add get_common_properties function to do extra property-based searches after identifying objects with extract
  - write function to identify contradictory information (retracted studies, false information, conspiracy theory (anti-vax), opinion) & selecting least likely to be false
    - this will be useful when youre pulling non-research study data, like when youre looking up a metric or compound if you dont find anything on wiki
  - write function to rank & identify authoritative sources (wiki is more trustworthy than a holistic or commercialized blog based on editing metadata)
  - add function to test chemical reactions: https://cheminfo.github.io/openchemlib-js/docs/classes/reaction.html
  - fill in keywords & patterns for objects (strategies/mechanisms used by an organism/on a compound)
  - function to predict a compound for a pathogen/condition requires data:
      - compound & pathogen attributes (compound metadata like metabolism/dose/interactions/effects)
      - variable/state impact (gene expression)
      - interaction rules with expected object types (in the bloodstream if taken orally, in the lungs if inhaled)
      - sub-components that could be altered through interaction to neutralize its functionality
      - dependency scope (volume of layers of relevance)
      - add identification functions:
          - types (['structure', 'life form', 'organic molecule'] from 'protein')
          - topic/problem domain
          - objects (nouns like 'protein')
          - components (topical nouns that are found in another topical component, like organelles of a cell)
          - attributes (attribute metric/feature nouns like 'toxicity')
          - functions (verbs like 'ionizing', 'activate', inputs/outputs like subject/predicate nouns)
          - variables (function inputs like subject/modifier nouns)
          - then test on bio systems:
            - "adjacency as a definition of relevance can be used as a way to derive paths" + "path optimization can be used to get a drug to a location in the system"
            - "isolate a pathogen cell before destroying it so it cant communicate info about what destroyed it to other pathogens to help them evolve resistance"
      - functions to determine:
        - position/role in a system 
        - function type associated with its core functions (change rules, boundary rules)
        - emergent effects in edge cases, rule change states, & interacting with other system layers
        - solution via conceptual route
      - function to derive core component functions for any system - then you can write functions to:
        - determine equivalent functions or more optimal version of a function
        - determine function intent
        - alter core functions used to alter function intent
        - when generating solutions, change core functions to vary to describe any function set that builds any other function set in a system
          - set of binding/interaction/priority functions for element atoms

# Conceptual

  - algorithm to decide when to use interface query, when to standardize to a particular interface, and when to stack interfaces to isolate variance by applying one interface to another
    - example: analyzing priority direction once you standardize to variance interface

  - algorithm to decide when its time to retire an interface:
    - when the exploit, optimization, & variance opportunities have been exhausted using all possible combinations of components & core functions, so that the interface has fulfilled its potential usage
    - if there is external system variance, another interface may develop to host the remaining variance

  - useful for computing attribute & variance flow as well as flow between flow of energy into measurement delivery (optimize energy distribution between particle position/spin attributes according to measurement limits)
    https://en.wikipedia.org/wiki/Partial_differential_equation

  - surprising patterns often come in the form of:
    - compounding patterns that go unmeasured (black swan pattern)
    - compounding patterns that are measured in ways that they dont vary from expected patterns (different dimension as a host of variance)
    - compounding patterns that are measured in ways that vary from expected patterns but not at point of measurement (wave function & line intersection)

  - expectation vacillation is optimized when neither extreme is expected permanently & expectations gravitate toward local inflection or threshold points
    - expecting evil & expecting sainthood are both sub-optimal in most situations, 
      whereas expecting moderation is usually more useful bc it allows more freedom, and more freedom allows more self-optimization than using forced optimization rules, 
      which change slower than local (self) optimization rules

  - give example of functionality positioning: 

    - high-traffic routes will involve more adjusted functions
    - gaps in routes (such as boundaries) will allow functional modules to evolve
    - progress & computational capacity of the network can be determined from connection ratios:
      - hub-to-hub distance, maximum trajectory distance, etc

  - give example of variance transforms

    - adding a new interface for variance to expand into
      - changing existing interface used as a filter to hold constant
      - adding new interactions to allow the expansion of new variance

    - moving variance into other interfaces to make the problem more solvable
      - rather than a species evolving into a new set of sub-species, host the variance in epigenetics rather than inherited mutations

  - brain learns through various reward models:
    - short term rewards: storing useful information
    - long term rewards: storing useful functions
    - adaptive/reusable rewards: storing functions by relevance or abstraction
    - on-demand rewards: storing function-generating methods/interfaces
    - reward potential-maximization: storing derivation method core functions

  - what does emotion interface map to?
    - emotion functions involve:
      - assigning cause
      - directing intent 
      - perspective/layer switching
    - emotions are used as a cause, a standard, and an output, allowing for variance injection due to current lack of measurability
    - they can capture sub-nets or trajectory nets on the interface network

  - apply your other solution to election security:
    - given a range of expected votes in a category, how much does final result deviate from expected votes

      - if an attribute like intelligence is associated with a particular vote, and outcome count deviates from known attribute count to this degree, what set of ratios of deviation can be attributed to noise and what percentage to interference/fraud?

      - what is the path between the determining attribute (intelligence) & the output concept (interference/fraud)?
        - is lack of intelligence a few transforms away from the output concept
          - meaning, if stupidity is associated with voting for the x party, is interference actual fraud given that its still indicating the expected stupidity rates in population?
          - or if stupidity is associated with voting for the x party, is the result inevitable, regardless of how it's achieved (they wouldve voted the same way as the fraudster tricked them into voting or artificially chose their vote with data manipulation)

  - navigation model examples:
    - predict variance sources & ratios:
      - type/shape/movement/interaction interface variance sources:
        - is an object that moves in restricted directions likely to be sentient life (delivery robot)
        - can sentient life move in restricted directions (drug addict, multi-tasking, drunk)
        - can interactions predict movement types (waving across street, looking at phone, predicting drivers' moves, checking street for cars/behind car before crossing/backing up)
        - are objects of a certain shape or size associated with different movement types (small objects are faster, larger objects have more obvious momentum physics, smaller objects likelier to be capable of flight)
      - assumption/hypothesis interface variance sources
        - are assumptions of a certain variance level given a certain minimum information associated with prediction accuracy
        - are multiple contradictory starting hypothesis associated with higher prediction accuracy, if so, with what degree of variance between hypotheses?

  - give example of calculating which variable resolutions can be postponed & solved later with information acquired in subsequent analysis
  - give example of framing variance with different interfaces to highlight predictive information or information that can be structured on other interfaces that simplify the problem
    - the common goal of 'variance reduction' which maps to 'applying a standard' may not always be the right first step, as 'variance expansion' can offer potential to identify different component interfaces that would highlight hidden variable set ranges
      - example: if you expand or maintain variance, or frame it as a combination of variances simultaneously framable in multiple standards, you can identify hidden variable gaps where variable sets on different interfaces can fill the gap in a way that doesnt disrupt the original variance
      - identify example where variance cant be reduced to a common interface but is still solvable with a set of interface positions/trajectories

  - give example of error types mapped to structural deficits

  - symmetries as a source of misclassification error and efficient/cooperative/optimal attributes
    - symmetry as a source of limits to restrict variance with
    - the limits can be common attributes/intents, around which variance develops
      - separation/delegation symmetry
      - retaining backup alternatives
      - organization
      - abstraction 
      - uniqueness 

    - symmetries are also where to look for key differentiating factors
      - the range allowed by a symmetry exists for a reason - to explore possible utility of variance in that range 

    - symmetry types:
      - functional symmetry: only restricted to find an optimal value, after which it's set to that value (tail shape) or the variance is reduced (remove tails with new design)
      - random symmetry: insignificant except in that it displays common patterns or efficiencies (independent evolution of intelligence across diverging species)
      - interface symmetry: allows enough variance that it continues attracting compounding variance (mammal interface, brain interface)
    
    - some symmetry types should be removed from data set bc they dont add variance but allow it to develop

    - you can also calculate the variance types to look for given the structure of a symmetry network
      - given symmetries powering an interface, predict which variable types to look for that could develop in that structure, then try to identify which phase of interface development its at
        - "given uniqueness and functional symmetries powering the identity interface, predict which variables to look for that could develop, then identify phase of development"
          
          - given that you need certain features in order to exist (a way to breathe, a way to sense, a way to communicate, a way to identify other people),
            predict which variables are likely to vary in the human interface, then identify phase of development
            - within the scope of features that can vary without impacting required survival functionality, senses for example can vary while still performing communication functions and are likely to vary to introduce testing of utility in different values, so senses are likely to vary
            - predict which phase of evolution the human interface is at: given the level of variance & optimization achieved on the human interface as well as variance trends, the human interface is likely reaching another explosion of evolution, given how much optimization potential has already been exploited in its existing interfaces (organ differentiation, sense optimization, neuron automation)
          
          - so in order to identify one human from another, look for sense variance (facial identification) as well as variance in the interfaces powering existing variance (organ differentiation, sense optimization, neuron automation, etc) when that expected variance occurs (may take the form of variance on other interfaces with less variance (like cellular structure variance, cellular communication variance, gene repair variance) but are likely to be connected by a functionality interface (reusing/sharing/finding functionality) as an identifying feature of the human interface, which now is struggling to be identified compared to its automatons, so efficiencies in processing are the next likely variance-hosting interface, if humans have to compete with machines for identity for example

    - you can also identify symmetry paths mapping to variance levels

      - in the example of muffin vs. chihuahua, the symmetry paths of a muffin:
        - common shape (humans gravitate toward simple common shapes so theyre likelier to create tools mimicking these shapes)
        - distribution (humans can understand concepts like optimization, and one implementation of optimization in the muffin interface is distribution of differentiating components)
        - humans have sensory preferences in common (texture, flavor), as some flavors are still useful indicators of healthy food (how dogs crave grass & other things when they're sick)
          so sensory attributes like texture & flavor are likely to converge for food intended as comfort food

      - lead to different variance levels than the symmetry paths of a chihuahua
        - distribution (dogs' sensory organs are distributed)
        - communication (dogs sensory organs need to communicate with each other (the mouth can detect if the nose is ok, the nose can detect if the eyes are ok, the ears can tell the eyes if dangerous fists are imminent)
        - backup alternatives (some organs' functions are so important you need a backup - eyes, kidneys, ears, mouth/nose as a possible airway, lungs)
        - uniqueness (domesticated dogs are treated like humans with identities, and sensory organs are likely to produce potentially identifying uniqueness due to the lack of functionality present in some sensory variance)
        - functional variance (some dog sensory organ configurations have utility value)

      - so you can expect variance in the chihuahua that the false chihuahua cannot provide
        - the chihuahua may vary in ways that a muffin cant given their symmetry paths
        - the chihuahua will vary along its facial interface (two eyes, mouth, nose) but the muffin distribution of components will be more random, for texture/flavor optimization
        - the symmetries making it unclear which is which include:
          - size (chihuahua face and muffin are often the same size)
          - the fact that visually measurable ingredients are often used in muffins (chocolate chip) and happen to mimic color scheme
          - the fact that there are only so many positions a chocolate chip can take, given its size, 
          - the fact that the baker will optimize the distribution metric with pretty good accuracy most of the time, meaning its likely to mimic a chihuahua face
        - these symmetries can be used as points for variable reduction (ignore size, ignore color scheme, etc)
        - the routes to these symmetries are insignificant in terms of the identity interface

      - the ways that significant symmetry paths vary can determine possible classification errors
        - chihuahua eyes can be shiny in different light environments
        - muffins may be coated with sugar, which may mimic the attribute 'shiny' used as a way to differentiate between muffin/chihuahua
        - this is a false symmetry, not indicating variance around a common interface, but can still be incorrectly identified as a legitimate symmetry (similarity in shininess means similar class)

      - these provide a point of variance reduction
        - shininess is not significant given that a distortion of an attribute in one class can mimic the naturally developed, identifying attribute in another class, so remove shininess from data set
      
      - therefore given the set of attribute values that can mimic each other, the insignificant symmetries, and the variable potentials left by the differences between significant symmetry paths, you can calculate which variables in a problem space will be capable of having predictive value
        - after removing false mimicking symmetries like shininess, insignificant symmetries like color schema, common symmetries like distribution, you have a limited set of differentiating features left:
          - muffin chocolate chips wont have tear ducts
          - chihuahua eyes wont have chocolate chip texture
        - therefore if you determine that your data set supports the level of detail required for these remaining attributes, you can use them as a predictor

      - given that a classification problem usually gets more difficult in time, you can also predict ways it will change to reduce the differentiating variables
        - they may start using candy that looks more like chihuahua eyes
        - they may start messing with the data set to lure the model into a false sense of security (using raisins which are easier to differentiate) so the actual data is more difficult to differentiate (chocolate chips)
        - this is a problem of identifying intent to misclassify, which means predicting symmetries in other objects that can be used to trick the algorithm into creating an inaccurate model
        - if an evil person wants to fool a movie theatre security ai into believing they are bringing their dog to the movie and not an evil reasonably-priced or homemade snack, there are many ways they can do so, but these ways are predictable & involve insignificant symmetries between classes (similar color, similar shape type)

        - how do you make your algorithm robust against these evil tricksters?
          - you need to be able to identify insignificant symmetries & distortion functions that can exploit them

      - signal layers can be optimized by order
        - images can be altered to highlight differentiating attributes so that bigger filters (splitting more data) are positioned first

    - standard nn structure has unidirectional causal support 
      - the collapse of sub-networks into a feature on a network with nodes to classify has patterns
      - the transformation of a classification network (containing nodes representing species) into a set of causally connected networks (containing nodes represent symmetries, types, patterns, functions)
        can be done if the data set contains a minimum of information needed to derive variance potentials, symmetry paths, type hierarchies, etc
      - the physics of attribute accretion into types can also be used to derive the set of networks building a classification network
      - if you have x variables on one interface and y variables on another interface, you can derive the network set to build a classification model for the classification network
      - the standard nn supports accreting attributes into types
      - it should also be able to support removing attributes to reveal differentiating factors that are not mimickable with distortion functions or random symmetries possible in the mimicking class
      - distortion functions are the first line of attack in reducing the set of identifying factors

    - what about networks with multidirectional causal support?
      - for example:
        - a classification problem that impacts the classification potential
        - an assumption set that changes the type evolution
        - a type structure that determines the attributes that can accrete in it, rather than only the attributes determining the type
          - example: given that a type has a set of symmetries around which variance accretes up to a limit, which attribute types are possible & which attribute probabilities occur given the level of variance allowed by the initial attribute set?
            - given that a level of variance occurs, what is the likelihood that a backup alternative variable will evolve?
            - "given that a particular sense is inherently limited but the demand for senses is not, what is the likelihood that an alternate way of sensing will evolve"

        - other layers of cause can occur before the type influences the attribute set - like a type that influences filters/symmetries that generate attributes

        - the relationship between attributes & types is multi-directional in cause, not to mention other relationships between interfaces

        - where is the potential for feedback from types-to-attributes in the network structure?
      - a species is overloaded with variance the more its compared to another mimicking species, or the mimicking species adopts variance to become a better mimicker
      - as attributes stabilize into types, its likelier that more interfaces of variance will develop that are not captured by those attributes

      - the relationship between the type/pattern/priority interfaces & variance-capturing variables is not unidirectional (one network builds the other, not the other way around or both)
      - however this network structure can often be used to capture the collapse of attribute sets into types, because types are by definition a collection of attributes, unless:
        - those types are in a state of convergence/divergence that isnt captured by the data or algorithm
        - some attributes have an insignificant symmetry with the identifying attributes (illusory correlation)
        - some attribute variance isnt captured by data set (chihuahua that has been in battle and lost some of its identifying attributes)
      - also there is a trajectory on the interface network that can build the identifying variable set (variance remaining after reduction by distortion functions & other insignificant symmetry types)
        which can take the form of a specific interface built for that problem type (filter out attributes that dont differentiate between these classes, leaving the differentiating attributes for comparison)
        this is what is called the prediction model/function, but usually its built from raw attributes than alternate interface attributes, which may be derivable from a data set that cant capture full identifying variance
      - an interface-building query for a problem type, integrated with a network algorithm, can therefore be a faster approach than using an information filtering network on its own
        similar to standardizing problems to optimal transport problems (how do you get to the target information state given initial information)
      - there may be interim interfaces that are more useful than raw data to determine a collapsed interface
        rather than using feature data to determine type, use feature pattern & feature evolution pattern data to determine type & type state
      - there may also be interfaces that allow variance in likelier patterns than another interface
        rather than using the priority or pattern or type interface, the function interface may capture more variance if the classes being identified are in a state of change

  - give example of map from intent to structural algorithm design
  - give example of mix & match interface assembly given the problem of 'model identification'
  - explainability as a space limited by inferentiable attributes from data set
  - threshold mechanics for threshold value selection
  - symmetry => interface 
  - min info => selection of structural layer
  - p-hacking 
  - nearest neighbors
  - give example of structuring problem in a certain format (optimal transport)
  - give example of matching structure 
  - give example of mapping problem semantically
  - choosing bio interfaces including electricity as components with a common language
  - discuss how sub-interfaces can collapse into a super interface (change interfaces) as the default shape of interfaces can be shapes other than networks
  - frame common problems with the standard of variance vs. time (recurrent nn)
  - examine whether new concepts (gaps in network rules) evolve once structure is applied to prior concepts 
  - all problem-solving automation methods have a variance assignment, allowing for variation to be explored in a certain location 
  - you can either map problems to fit that structure or design new automation methods based on the variance gap necessary to solve a problem

  - make diagram of potential matrix to display the concept
    - map parameter sets to potential matrix shapes 

  - add error-generation
    - add diagrams for error types:
      - misalignment
      - assumptions without supporting logical/information links
      - incorrect position/function/structure/scope/limit/range/definition

  - add causal type identification for known solutions:
    - causal types: direct/indirect, multiple/alternate, hierarchical, replaceable/unique, generatable/emergent
    - why did something work? because of its:
      - structure (which piece - the imidazole)?
      - interactions
      - other attributes
      - similarities

  - add variable-selection example with separate alts having equivalent outputs:
    - if one alt is disabled, then it would give a false result for anyone checking it for ability to impact the output, even though the alternative was being variably used instead

  - function to identify & remove common article intents with high probability of falsehood to reduce it to just facts
  - add intent matching so you can compare treatment relationships with article intents to see if its actually a sentence with a treatment in it
    - finish treatment failure condition - make sure it adds nothing if theres no treatment in the article - this is related to intent function
  - use distortion patterns of entities like atlases, templates, solution progressions to form a compressed version of the host system
    https://techxplore.com/news/2019-11-medical-image-analysis.html
  - add stressor language patterns
  - for queries of functions like "disable a gene", you can include intent & function metadata to point to sets of compounds that could do the required edits:
    - find compound (protein, enzyme, etc) that unfolds DNA
    - find compound that modifies (edits, activates, removes) the gene once unfolded as specifically as possible 
      (can be a compound with a cutting subcomponent at the right length to target the dna if you can bind it to the first or last gene with another compound)
    - find compound with function = "refolds DNA"
    https://medicalxpress.com/news/2019-12-common-insulin-pathway-cancer-diabetes.html

  - now that youve automated problem-solving, there are still things with variable implementations allowing for innovation in this problem space:
    - selecting functions (solution methods)
    - selecting limits (metrics)
    - prioritizing problem solving order

# ML
  - the full data set should have numerical categories indicating condition(s) treated in the output label so it can be separated into sub-sets by condition treated
  - incorporate stacked autoencoders to leverage unsupervised learning to get initial weights
  - incorporate cosine loss rather than categorical cross entropy
  - add recurrent nn example code that can be copied & plugged in without modification

# Diagrams

- make diagram for variable accretion patterns
- finish diagrams for specific concepts, core functions, concept operations, ethical shapes
- finish informal fallacy diagrams: https://en.wikipedia.org/wiki/List_of_fallacies
- consider using dimensionality reduction as a way to identify abstract patterns & functions to explain common deviations from patterns
    https://miro.medium.com/max/1659/1*nQrZmfQE3zmMnCJLb_MNpQ.png
    https://towardsdatascience.com/step-by-step-signal-processing-with-machine-learning-pca-ica-nmf-8de2f375c422
- use this or similar as example when describing current state of problem solving: 
    https://miro.medium.com/max/462/1*X7dQgs1gsJ0Sktz3t7J21Q.png
    https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be

# Questions
  - are pathogen receptors/membranes unique enough that you could design a substance to artificially bind with them to deactivate or puncture the membrane without impacting other structures?
